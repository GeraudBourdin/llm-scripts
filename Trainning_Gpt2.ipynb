{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkx8ae5LDgJtbXmNwOnK9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeraudBourdin/llm-scripts/blob/main/Trainning_Gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "LhK5fhFaPNvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Définition du model pour l'ensemble de l'execution de cette feuille.\n",
        "## https://huggingface.co/openai-community\n",
        "\n",
        "MODEL = \"gpt2\";        ##  openai-community/gpt2\n",
        "#MODEL = \"gpt2-medium\";      ## openai-community/gpt2-medium\n",
        "#MODEL = \"gpt2-large\";       ##  openai-community/gpt2-large\n",
        "#MODEL = \"gpt2-xl\"; ##  openai-community/gpt2-xl\n"
      ],
      "metadata": {
        "id": "43SIFulUum1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUO3K2Y4t5ph"
      },
      "outputs": [],
      "source": [
        "## Installation des  Libraries\n",
        "!pip install transformers --use-deprecated=legacy-resolver\n",
        "!pip install datasets\n",
        "!pip install GPUtil\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import des librairies\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from numba import cuda\n",
        "from datasets import load_dataset\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, default_data_collator"
      ],
      "metadata": {
        "id": "huqXTvvWuCAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL,\n",
        "                                        pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device) #optionnel"
      ],
      "metadata": {
        "id": "0VFow3MhvEwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explication: Tokenizer\n",
        "Aucune utilité concernant notre but final."
      ],
      "metadata": {
        "id": "Zf2KPX9kyk8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Quelques informations concernant le model loadé\n",
        "print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n",
        "print(\"default maximum supported sentence length is 1024 is {}\".format(tokenizer.max_model_input_sizes))\n",
        "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
        "tokenizer.max_model_input_sizes"
      ],
      "metadata": {
        "id": "adFb_q_4wta3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## les informations du tokenizer\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "yZZIn-g9xGTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Création d'une série de tokens en fonction d'une phrase\n",
        "sentence = 'I am a PHP Developer'\n",
        "input_ids  = tokenizer.encode(sentence,\n",
        "                              return_tensors = 'pt')\n",
        "print('encoded : ')\n",
        "print(input_ids)\n",
        "\n",
        "\n",
        "print(\"\\r\\n\")\n",
        "## decode tokens :\n",
        "print('décoded : ')\n",
        "print(tokenizer.decode(input_ids[0][3]))"
      ],
      "metadata": {
        "id": "gIqn8WI6xvGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explication: Génération de texte"
      ],
      "metadata": {
        "id": "eL_abLze3dHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Search**: Le mot suivant est prédit en fonction de sa probabilité la plus haute\n",
        "\n",
        "**Beam Search**: Plusieurs mots suivants sont prédits, le model détermine dans ces mots quel sera le meilleur ( `num_beams=5` )\n",
        "\n",
        "**Random sampling**: le mot suivant est déterminé de maniere aléatoire. On utilise `do_sample=True`. Mais ça peut conduire à des incohérences. On utilise alors le parametre temperature pour faire la balance entre la probabilité haute et basse du meilleur mot suivant.\n",
        "\n",
        "**Top-K Sampling**: On se concentre sur les K mots suivant en éliminant toutes les mots a faible probabilité.\n",
        "\n",
        "**Top-P Sampling**: il se concentre sur le choix du plus petit ensemble de mots pour lequel la probabilité cumulée dépasse un seuil donné Pour implémenter l'échantillonnage Top-P, définissez simplement \"top_k\" sur 0 et spécifiez une valeur pour \"top_p\"."
      ],
      "metadata": {
        "id": "xXhsB7QRPXxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy Search ( Le mot suivant est prédit en fonction de sa probabilité la plus haute )\n",
        "greedy_output = model.generate(input_ids,\n",
        "                               max_length=100,\n",
        "                               no_repeat_ngram_size=2)\n",
        "\n",
        "for i, output in enumerate(greedy_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output, skip_special_tokens=True)))\n",
        "    print('')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YddwRqO-zCjD",
        "outputId": "aec0f331-4fec-48c3-b981-10ada60dd7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I am a PHP Developer and I am very passionate about the PHP language. I have been working on PHP for over 10 years and have worked on many different projects.\n",
            "\n",
            "I have a passion for the web and want to make it better. So I decided to start my own company. It is called \"The PHP Project\".\n",
            ".php\n",
            " (The name is a reference to the popular PHP programming language, PHP. The PHP project is based on the original PHP, which was developed by the...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Beam Search:\n",
        "## - des \"beams\" sont produit correspondant à n potentiels mots suivants appelés hypotheses.\n",
        "## - Plus il y a d'hypotheses à prédire a chacune des étapes plus il y a de calculs.\n",
        "## - il faut trouver le bon ratio entre la pertinence et la lourdeur de la génération de phrases.\n",
        "beam_output = model.generate(input_ids,\n",
        "                             max_length = 100,\n",
        "                             num_beams=5,\n",
        "                             num_return_sequences=3, ## le nombre de séquences retournées doit être < à num_beams\n",
        "                             no_repeat_ngram_size=2,\n",
        "                             early_stopping=True)\n",
        "\n",
        "for i, output in enumerate(beam_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output, skip_special_tokens=True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61ur5fzGzEIl",
        "outputId": "e08a870d-1eb6-4ca1-9339-d29527c803ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I am a PHP Developer and I have been working on this project for a few years now.\n",
            "\n",
            "In this post I am going to show you how to use PHP 5.5 to build your own PHP application. I hope you will enjoy this tutorial as much as I enjoyed writing it. If you have any questions or comments, feel free to leave them in the comments below....\n",
            "\n",
            "1: I am a PHP Developer and I have been working on this project for a few years now.\n",
            "\n",
            "In this post I am going to show you how to use PHP 5.5 to build your own PHP application. I hope you will enjoy this tutorial as much as I enjoyed writing it. If you have any questions or comments, feel free to leave them in the comments section below....\n",
            "\n",
            "2: I am a PHP Developer and I have been working on this project for a few years now.\n",
            "\n",
            "In this post I am going to show you how to use PHP 5.5 to build your own PHP application. I hope you will enjoy this tutorial as much as I enjoyed writing it. If you have any questions or comments, feel free to leave them in the comment section below....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Random sampling,\n",
        "## le mot suivant est déterminé de maniere aléatoire.\n",
        "## do_sample=True peut rendre le texte incohérent\n",
        "## Pour éviter cela on utilise \"temperature\" qui fait la balance entre la priorité haute et basse du meilleur mot suivant.\n",
        "\n",
        "random_output = model.generate(input_ids,\n",
        "                               do_sample=True,\n",
        "                               max_length=100,\n",
        "                               top_k=0,\n",
        "                               temperature=0.8)\n",
        "\n",
        "for i, output in enumerate(random_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWUn6DYT07RN",
        "outputId": "5bc81d26-07f5-425d-abfa-ecc028ccd348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I am a PHP Developer, so it was easy to figure out the minds behind all of my projects. However, I never got around to implementing the frameworks that I was studying for. So for example, the first time I worked on a PHP project I didn't have the skills to deploy to a web server. When I did that, I was at a loss how I had to implement it.\n",
            "\n",
            "After a lot of work, I managed to figure out how to make a remote web server...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Top-K Sampling\n",
        "## On se concentre sur les K mots suivant en éliminant toutes les mots a faible probabilité.\n",
        "## Donc dans un random sampling on va éléiminer la partie basse de la temperature ( balance entre probabilité haute et basse)\n",
        "## donc top_k permet de donner un nombre de motsprincipaux que nous allons inclure dans la distribution de probabilité conditionnelle.\n",
        "\n",
        "top_k_output = model.generate(input_ids,\n",
        "                              do_sample=True,\n",
        "                              max_length=100,\n",
        "                              top_k=50)\n",
        "\n",
        "for i, output in enumerate(top_k_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIH68sS54d4Y",
        "outputId": "87445151-6bba-4b45-8d69-326e3783be8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I am a PHP Developer and I know all about PHP and that's what I do at Microsoft. I know that PHP on Windows and OS X is a very modern OS which means that the PHP you are writing will probably become much smaller. It also means that PHP 3.8 is less and less likely to use the same language. PHP is not only one of the most widely used programming languages, but also because it is written in PHP and it is fast!\n",
            "\n",
            "So how do you know...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Top-P Sampling: Au lieu de sélectionner les k mots les plus probables,\n",
        "## il se concentre sur le choix du plus petit ensemble de mots pour lequel\n",
        "## la probabilité cumulée dépasse un seuil donné, noté « p ».\n",
        "## Dans cette méthode, toute la masse de probabilité est ensuite déplacée vers les mots de cet ensemble.\n",
        "## La principale distinction entre l'échantillonnage Top-K et Top-P réside dans la flexibilité de la taille de l'ensemble.\n",
        "## Dans l'échantillonnage Top-K, la taille définie reste fixe, tandis que dans l'échantillonnage Top-P, la taille peut varier.\n",
        "## Pour implémenter l'échantillonnage Top-P, définissez simplement \"top_k\" sur 0 et spécifiez une valeur pour \"top_p\".\n",
        "\n",
        "\n",
        "top_p_output = model.generate(input_ids,\n",
        "                              do_sample=True,\n",
        "                              max_length=100,\n",
        "                              top_p=0.8,\n",
        "                              top_k=0)\n",
        "\n",
        "for i, output in enumerate(top_p_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gTyYCs04mn7",
        "outputId": "72d8627a-a0eb-4e54-91c4-46c8c56972ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I am a PHP Developer, I'm passionate about creating performance driven apps. However, I am also a PHP-dev, which is why I need to get to know the complex programming language and how it can help me learn.\n",
            "\n",
            "A big part of that programming knowledge comes from mastering PHP. If you have been playing with PHP for a long time and have been using it for some time, then it is very easy to learn and can be used in a variety of ways.\n",
            "\n",
            "Now...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons la flexibilité d'utiliser les techniques d'échantillonnage **Top-K** et **Top-P** dans notre approche. Cette combinaison permet d'atténuer l'inclusion de mots inhabituels ou à faible probabilité tout en permettant une taille de sélection dynamique. Pour implémenter cela, nous devons simplement spécifier des valeurs pour « **top_k** » et « **top_p** ». De plus, si nous le souhaitons, nous pouvons incorporer le paramètre de **temperature** initial."
      ],
      "metadata": {
        "id": "IOt3DKqF7HVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_p_outputs = model.generate(input_ids,\n",
        "                                 do_sample=True,\n",
        "                                 max_length=2*100,\n",
        "                                 top_k=50,\n",
        "                                 top_p=0.85,\n",
        "                                 num_return_sequences=5)\n",
        "\n",
        "\n",
        "for i, output in enumerate(top_k_p_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqnw8E-D7Uv3",
        "outputId": "2d0a7343-b973-4eb4-84df-c3bfebcf3086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: I am a PHP Developer and I am the CEO of a company which makes a lot of amazing products. We are a community driven organization. I am happy to have your support.\n",
            "\n",
            "The people who make and sell this website. The people that support us, the people that make and sell our products, the people who make and sell our content. This is what we need to build something special.\n",
            "\n",
            "How can I help?\n",
            "\n",
            "We are working hard on our brand new web site. If you're interested in joining us, there's no better way to do it than to sign up for our mailing list here....\n",
            "\n",
            "1: I am a PHP Developer and Developer and I am a Certified Composer. If you are not a PHP Developer, please do not read this guide because I want to help you understand the PHP concepts.\n",
            "\n",
            "\n",
            "There is no real magic trick or trick to the PHP programming language.\n",
            "\n",
            "\n",
            "PHP is simply a language that is used to develop applications. It does not have to be the same to do programming. You can create apps, read documents, edit documents, build websites and even write websites. It is just that there is nothing like PHP or C++ in it and that this is the best PHP language you can learn.\n",
            "\n",
            "\n",
            "PHP is the perfect language to learn about programming. It is the best programming language to learn PHP. It is the best programming language to learn PHP. It is the best programming language to learn PHP.\n",
            "\n",
            "\n",
            "PHP is written in JavaScript. It does not have the ability to render HTML. You can't write your own HTML files. You cannot...\n",
            "\n",
            "2: I am a PHP Developer, so I have to be sure I am not being used to using these tools. The main point of this tutorial is to tell you about the various PHP tools that you can use to create database tables and other things like that, so don't be confused.\n",
            "\n",
            "First off, let's look at the examples. We will use the PHP Database Analyzer to get an understanding of all of the PHP tools, what to use, how to configure them, what they can do. You can read more about the PHP Database Analyzer here.\n",
            "\n",
            "The PHP Database Analyzer lets you run PHP on your MySQL server with the following settings:\n",
            "\n",
            "$mysql = New-Object System.IO.File(\"/var/log/mysql\"); $db = new-object System.IO.File(\"/var/log/db\"); $db->exec($this->db->Execute($this)); $db->sql($this->db->Exec...\n",
            "\n",
            "3: I am a PHP Developer. As such, I have a lot of knowledge on the language and I love the tools and technologies used by programmers.\n",
            "\n",
            "I want to share my experience with you as you will be the first to learn about PHP. We are going to talk about different technologies, different languages, and some basic concepts.\n",
            "\n",
            "I'll start by introducing some of the most popular libraries:\n",
            "\n",
            "Safari\n",
            "\n",
            "Pagetty\n",
            "\n",
            "PHP Express\n",
            "\n",
            "PHP Basic\n",
            "\n",
            "Pagetty 2.0\n",
            "\n",
            "PHP Express 3.0\n",
            "\n",
            "PHP Basic 1.0\n",
            "\n",
            "PHP Basic 2.0\n",
            "\n",
            "PHP Express 3.0\n",
            "\n",
            "PHP Basic 2.1\n",
            "\n",
            "PHP Basic 3.0\n",
            "\n",
            "PHP Basic 2.2\n",
            "\n",
            "PHP Basic 2.3\n",
            "\n",
            "PHP Express 3.0\n",
            "\n",
            "PHP Express 4.0\n",
            "\n",
            "PHP Express 5.0\n",
            "...\n",
            "\n",
            "4: I am a PHP Developer, PHP developer and developer of PHP for Windows, OS X and Linux. I work with a variety of organizations and have worked with PHP software development platforms like C and PHP for various projects since 2003. I am most recently a PHP developer, working with the open source Open Source Development Platform. I am a strong advocate for security of PHP and PHP for Windows.\n",
            "\n",
            "I am a PHP Developer, PHP developer and developer of PHP for Windows, OS X and Linux. I work with a variety of organizations and have worked with PHP software development platforms like C and PHP for various projects since 2003. I am most recently a PHP developer, working with the open source Open Source Development Platform. I am a strong advocate for security of PHP and PHP for Windows. PHP Professional\n",
            "\n",
            "I am a PHP developer, PHP developer and developer of PHP for Windows, OS X and Linux. I work with a variety of organizations and have worked with PHP software development platforms like C and PHP for...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilisation d'un dataset"
      ],
      "metadata": {
        "id": "A8psk6JA7bVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## On défini le dataset qu'on va vouloir exploiter\n",
        "dataset_name = \"tiny_shakespeare\" ## https://huggingface.co/datasets/tiny_shakespeare ne contient qu'une seul ligne donc rapide.\n",
        "\n",
        "\n",
        "cache_dir = \"lm_dataset/\"\n",
        "datasets = load_dataset(dataset_name, cache_dir=cache_dir)\n",
        "\n",
        "\n",
        "print(datasets)"
      ],
      "metadata": {
        "id": "l8h3A3px7hwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## On applique le tokenizer au dataset\n",
        "## le dataset contient 3 parties : train / validation / test\n",
        "## train contient la colonne \"text\"\n",
        "\n",
        "column_names = datasets[\"train\"].column_names\n",
        "## Bon c'est assez simple, si contient la conlonne \"texte\" sinon on prent la premiere par defaut\n",
        "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    output = tokenizer(examples[text_column_name])\n",
        "    return output\n",
        "\n",
        "tokenized_datasets = datasets.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Running tokenizer on dataset\")"
      ],
      "metadata": {
        "id": "cy3UJEqO7kgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## lorsqu'on travail avec de grosses données il faut scinder en \"chunks\" en fonction\n",
        "# du nobre max de token que peut prendre le model en context d'input.\n",
        "\n",
        "block_size = tokenizer.model_max_length\n",
        "## si jamais le context peut prendre plus de 1024 on bride tout de même a 1024\n",
        "## sinon on respect le max du model.\n",
        "if block_size > 1024:\n",
        "    block_size = 1024\n",
        "\n",
        "## On crée une fonction group_texts\n",
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()}\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    desc=f\"Grouping texts in chunks of {block_size}\")"
      ],
      "metadata": {
        "id": "y9okxFna87lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = lm_datasets[\"train\"]\n",
        "eval_dataset = lm_datasets[\"validation\"]\n",
        "\n",
        "training_args = TrainingArguments(output_dir = \"output/\",\n",
        "                                  per_device_train_batch_size=1,\n",
        "                                  num_train_epochs=50,\n",
        "                                  save_total_limit=1)\n",
        "trainer = Trainer(model=model,\n",
        "                  args=training_args,\n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset=eval_dataset,\n",
        "                  tokenizer=tokenizer,\n",
        "                  data_collator=default_data_collator)\n",
        "\n",
        "\n",
        "\n",
        "##### si jamis il y a cette erreur : Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1\n",
        "##### c'est qu'il faut redemarer l'environnement (Execution>redemarer la session et tout executer)"
      ],
      "metadata": {
        "id": "m5l2OEiC_hTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## nettoyage de la memoire\n",
        "\n",
        "gpu_usage()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31D59627OwPA",
        "outputId": "68673925-0f17-4dfb-8b0d-07e0d8e03f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## On lance l'entrainnement\n",
        "\n",
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ASiAGSGqSAMR",
        "outputId": "94a9cabc-ccb0-49c2-d6d6-849aa1304516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='14700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    9/14700 05:16 < 184:23:23, 0.02 it/s, Epoch 0.03/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()\n",
        "\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "m5t5OAD6SLR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2)\n",
        "\n",
        "ids = tokenizer.encode('One does not simply walk into',\n",
        "                       return_tensors='pt').cuda()\n",
        "\n",
        "\n",
        "\n",
        "################## GREEDY\n",
        "print('################## GREEDY')\n",
        "greedy_output = model.generate(ids,\n",
        "                               max_length=100,\n",
        "                               no_repeat_ngram_size=2)\n",
        "for i, output in enumerate(greedy_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "\n",
        "################## BEAM\n",
        "print('################## BEAM')\n",
        "beam_output = model.generate(ids,\n",
        "                             max_length = 100,\n",
        "                             num_beams=5,\n",
        "                             num_return_sequences=5,\n",
        "                             no_repeat_ngram_size=2,\n",
        "                             early_stopping=True)\n",
        "\n",
        "for i, output in enumerate(beam_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')\n",
        "\n",
        "################## Random Sampling\n",
        "print('################## Random Sampling')\n",
        "random_output = model.generate(ids,\n",
        "                               do_sample=True,\n",
        "                               max_length=100,\n",
        "                               top_k=0,\n",
        "                               temperature=0.8)\n",
        "\n",
        "for i, output in enumerate(random_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "################## Top-K Sampling\n",
        "print('################## Top-K Sampling')\n",
        "\n",
        "top_k_output = model.generate(ids,\n",
        "                              do_sample=True,\n",
        "                              max_length=100,\n",
        "                              top_k=50)\n",
        "\n",
        "for i, output in enumerate(top_k_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "################## Top-P Sampling\n",
        "print('################## Top-P Sampling')\n",
        "\n",
        "top_p_output = model.generate(ids,\n",
        "                              do_sample=True,\n",
        "                              max_length=100,\n",
        "                              top_p=0.8,\n",
        "                              top_k=0)\n",
        "\n",
        "for i, output in enumerate(top_p_output):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "################## Top-K and Top-P Sampling\n",
        "print('################## Top-K and Top-P Sampling')\n",
        "\n",
        "top_k_p_outputs = model.generate(ids,\n",
        "                                 do_sample=True,\n",
        "                                 max_length=2*100,\n",
        "                                 top_k=50,\n",
        "                                 top_p=0.85,\n",
        "                                 num_return_sequences=5)\n",
        "\n",
        "for i, output in enumerate(top_k_p_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(output,\n",
        "                                                 skip_special_tokens=True)))\n",
        "    print('')"
      ],
      "metadata": {
        "id": "Xc527FaDSRsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}