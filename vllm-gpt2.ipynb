{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoGETfm6vhVblxv7xtf5kz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeraudBourdin/llm-scripts/blob/main/vllm-gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avec VVLM"
      ],
      "metadata": {
        "id": "boCnT_0nKkiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "id": "8vuF2OFzzIFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EcoV4UozGal"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM\n",
        "\n",
        "prompts = [\"Hello, my name is\", \"The capital of France is\"]\n",
        "llm = LLM(model=\"gpt2\")\n",
        "outputs = llm.generate(prompts)\n",
        "\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avec le CPU seulement"
      ],
      "metadata": {
        "id": "I5ZkF8CQGN7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers"
      ],
      "metadata": {
        "id": "gsiVbRMqHUl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(0)\n",
        "\n",
        "tokenizer   = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model       = AutoModelForCausalLM.from_pretrained(\"gpt2\",return_dict_in_generate=True)\n",
        "model.to(device)\n",
        "\n",
        "input_text = \"The best thing about AI is its ability to\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "attention_mask = torch.ones(\n",
        "    input_ids.shape\n",
        "    , dtype=torch.long\n",
        "    , device=model.device\n",
        ")\n",
        "\n",
        "\n",
        "#output the probability of each out put words\n",
        "output = model.generate(\n",
        "    input_ids\n",
        "    , attention_mask    = attention_mask\n",
        "    , eos_token_id      = tokenizer.eos_token_id\n",
        "    , do_sample         = True\n",
        "    , output_scores     = True\n",
        "    , max_new_tokens    = 512\n",
        ")\n",
        "\n",
        "# only use id's that were generated\n",
        "gen_sequences = output.sequences[:, input_ids.shape[-1]:]\n",
        "generated_text = tokenizer.decode(gen_sequences[0],skip_special_tokens=True)\n",
        "\n",
        "print(\"################################################\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "rwZWvAfEGQo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avec un pipeline Huggingface"
      ],
      "metadata": {
        "id": "Oj1DZGxGKHDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "input_text = \"The best thing about AI is its ability to\"\n",
        "\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "text = generator(\n",
        "                  input_text\n",
        "                 ,max_length           = 512\n",
        "                 ,pad_token_id         = 50256\n",
        "                 ,num_return_sequences = 1\n",
        "                 )\n",
        "\n",
        "outputs = generator(\n",
        "                   input_text\n",
        "                  ,num_return_sequences = 1\n",
        "                  ,pad_token_id=generator.tokenizer.eos_token_id\n",
        "                  , max_new_tokens    = 512\n",
        "                  )\n",
        "\n",
        "\n",
        "for output in outputs:\n",
        "  generated_text = output['generated_text']\n",
        "  print(f\"{generated_text!r}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiEot24MKHu7",
        "outputId": "53c3f2dd-6f07-4ab7-b7eb-d2e487078af4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"The best thing about AI is its ability to do something simple and quick, but it's great to learn, use, test, optimize, and evolve to new patterns. I would love to see more AI learn from others, because they are more interesting. We are now in the era of deep learning, and many are working with it extensively. I think a lot of these technologies will be more useful in the future, so when we are just starting to learn how to drive, drive, drive, drive, you can start making some pretty cool things if we go along that way.\"\n"
          ]
        }
      ]
    }
  ]
}