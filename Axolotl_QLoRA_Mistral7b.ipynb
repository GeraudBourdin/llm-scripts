{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeraudBourdin/llm-scripts/blob/main/Axolotl_QLoRA_Mistral7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lARMRP9CiDPh"
      },
      "source": [
        "# Axolotl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g46zE80tiLww"
      },
      "source": [
        "# **Commandes utiles**\n",
        "\n",
        "**Finetuning**\n",
        "```\n",
        "accelerate launch -m axolotl.cli.train mistral-orca-config.yml\n",
        "```\n",
        "\n",
        "**Inference** le model que l'on vient de finetuné\n",
        "```\n",
        "accelerate launch -m axolotl.cli.inference mistral-orca-config.yml --sample_packing False\n",
        "```\n",
        "\n",
        "**Debug** Pour afficher les promtps sur lesquelles le modèle est en cours Finetuning\n",
        "```\n",
        "accelerate launch -m axolotl.cli.train mistral-orca-config.yml --debug_text_only --debug\n",
        "```\n",
        "\n",
        "**Debug** avec les la vidu des tokens --prepare_ds_only\n",
        "```\n",
        "accelerate launch -m axolotl.cli.train mistral-orca-config.yml --prepare_ds_only --debug\n",
        "```\n",
        "\n",
        "**Fusionner QLoRA** avec son model de reference\n",
        "```\n",
        "python3 -m axolotl.cli.merge_lora mistral-orca-config.yml --lora_model_dir=\"./finetuned-model\" --load_in_8bit=False --load_in_4bit=False\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmChoBYE9C8G"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud2aFMrSiAC3"
      },
      "outputs": [],
      "source": [
        "### sur jupyter\n",
        "\n",
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl\n",
        "%cd axolotl\n",
        "# !pip install torch==2.1.2\n",
        "# !pip install -r requirements.txt\n",
        "!pip install packaging\n",
        "!pip install -e '.[flash-attn,deepspeed]'\n",
        "!pip install huggingface_hub\n",
        "\n",
        "!git config --global credential.helper store\n",
        "!wandb login --relogin\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### sur colab\n",
        "!pip install torch==\"2.1.2\"\n",
        "!pip install -e git+https://github.com/OpenAccess-AI-Collective/axolotl#egg=axolotl\n",
        "!pip install flash-attn==\"2.5.0\"\n",
        "!pip install deepspeed==\"0.13.1\"\n",
        "!pip install huggingface_hub\n",
        "\n",
        "!git config --global credential.helper store\n",
        "!wandb login --relogin\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "oZdMcn6UfF6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ5jnFQpCcad"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB2zDSlHCfHd"
      },
      "outputs": [],
      "source": [
        "contenu = \"\"\"\n",
        "base_model: mistralai/Mistral-7B-Instruct-v0.2\n",
        "model_type: MistralForCausalLM\n",
        "tokenizer_type: LlamaTokenizer\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "  - path: /content/train.json\n",
        "    ds_type: json\n",
        "    type: context_qa.load_v2\n",
        "output_dir: ./qlora-out_customer\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "\n",
        "sequence_len: 8192\n",
        "sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "lora_target_modules:\n",
        "  - gate_proj\n",
        "  - down_proj\n",
        "  - up_proj\n",
        "  - q_proj\n",
        "  - v_proj\n",
        "  - k_proj\n",
        "  - o_proj\n",
        "\n",
        "wandb_project: partitech\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_name: partitech__customer_axolotl\n",
        "wandb_log_model:\n",
        "\n",
        "hf_use_auth_token: true\n",
        "hub_strategy: checkpoint\n",
        "hub_model_id: partitech/test_axolotl3\n",
        "\n",
        "\n",
        "gradient_accumulation_steps: 4\n",
        "micro_batch_size: 2\n",
        "num_epochs: 4\n",
        "optimizer: adamw_bnb_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.0002\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: auto\n",
        "fp16:\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: true\n",
        "\n",
        "loss_watchdog_threshold: 5.0\n",
        "loss_watchdog_patience: 3\n",
        "\n",
        "warmup_steps: 10\n",
        "evals_per_epoch: 1\n",
        "eval_table_size:\n",
        "eval_max_new_tokens: 128\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "\n",
        "resume_from_checkpoint: /content/qlora-out_customer/checkpoint-1083\n",
        "\"\"\"\n",
        "##### si besoin de resumer depuis le dernier checkpoint:\n",
        "##### resume_from_checkpoint: ./qlora-out_customer/checkpoint-xxxx\n",
        "\n",
        "\n",
        "with open(\"/content/partitech.yaml\", \"w\") as fichier:\n",
        "    fichier.write(contenu)\n",
        "\n",
        "!cat /content/partitech.yaml\n",
        "!head /content/train.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1TXneA2F-mp"
      },
      "outputs": [],
      "source": [
        "!accelerate launch -m axolotl.cli.train /content/partitech.yaml  --debug_text_only --debug"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Relancer un train pour continuer des epochs\n",
        "\n",
        "\n",
        "Pour relancer un epoch sur les fichier soit on recupere les fichier dans le repertoire checkpointxxxxx\n",
        "\n",
        "Soit si il sont sur huggingface on recupere les fichiers avec"
      ],
      "metadata": {
        "id": "gnClgbRkhJmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "\n"
      ],
      "metadata": {
        "id": "Ogcv3vfdwfOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository\n",
        "repo = Repository(local_dir=\"test_epoch4\", clone_from=\"partitech/test_axolotl2\")"
      ],
      "metadata": {
        "id": "LN5ZAb1CyNuG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyORpv/KBbS45xZ5Y8IC9ugj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}